{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Setup and Dependencies"
      ],
      "metadata": {
        "id": "NhxVO9VTs8gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Special tokens\n",
        "PAD, BOS, EOS, UNK = 0, 1, 2, 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B33qVDsynqK3",
        "outputId": "a926606f-dbb4-416a-e823-2281d09d0092"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "en_ur_pairs = [\n",
        " ('I am going to school', 'میں سکول جا رہا ہوں'),\n",
        " ('He is a doctor', 'وہ ایک ڈاکٹر ہے'),\n",
        " ('It is a beautiful day', 'یہ ایک خوبصورت دن ہے'),\n",
        " ('The book is on the table', 'کتاب میز پر ہے'),\n",
        " ('Are you okay?', 'کیا آپ ٹھیک ہیں؟'),\n",
        " ('I need to drink water', 'مجھے پانی پینا ہے'),\n",
        " ('We will go to the market tomorrow', 'ہم کل بازار جائیں گے'),\n",
        " ('Where do you live?', 'تم کہاں رہتے ہو؟'),\n",
        " ('My name is Mahnoor', 'میرا نام ماہ نور ہے'),\n",
        " ('Thank you', 'آپ کا شکریہ'),\n",
        " ('Close the door', 'دروازہ بند کر دو'),\n",
        " ('I need help', 'مجھے مدد کی ضرورت ہے'),\n",
        " ('Did you eat food?', 'کیا تم نے کھانا کھایا؟'),\n",
        " ('I watched a movie', 'میں نے فلم دیکھی'),\n",
        " ('It is raining outside', 'باہر بارش ہو رہی ہے'),\n",
        " ('I am tired', 'میں تھک گیا ہوں'),\n",
        " ('She is very intelligent', 'وہ بہت ذہین ہے'),\n",
        " ('We are all friends', 'ہم سب دوست ہیں'),\n",
        " ('Your shawl is very beautiful', 'تمہاری شال بہت خوبصورت ہے'),\n",
        " ('I have a dog', 'میرے پاس ایک کتا ہے'),\n",
        " ('This book is very interesting', 'یہ کتاب بہت دلچسپ ہے'),\n",
        " ('I like technology', 'مجھے ٹیکنالوجی پسند ہے'),\n",
        " ('Where are you going?', 'تم کہاں جا رہے ہو؟'),\n",
        " ('My parents are happy', 'میرے والدین خوش ہیں'),\n",
        " ('What do you want?', 'آپ کو کیا چاہیے؟'),\n",
        " ('I am watching you', 'میں تمہیں دیکھ رہا ہوں'),\n",
        " ('We are all learning', 'ہم سب کچھ سیکھ رہے ہیں'),\n",
        " ('This game was very fun', 'یہ کھیل بہت مزے کا تھا'),\n",
        " (\"My friend's name is Ali\", 'میرے دوست کا نام علی ہے'),\n",
        " ('What are you studying?', 'تم کیا پڑھ رہے ہو؟'),\n",
        " ('I have started learning a new language',\n",
        "  'میں نے نئی زبان سیکھنی شروع کی ہے'),\n",
        " ('Come with me', 'میرے ساتھ چلوں'),\n",
        " ('We will meet soon', 'ہماری ملاقات جلد ہوگی'),\n",
        " ('She cooks very well', 'وہ بہت اچھا کھانا پکاتی ہے'),\n",
        " ('This is my favorite color', 'یہ میرا پسندیدہ رنگ ہے'),\n",
        " ('How old are you?', 'تم کتنے سال کے ہو؟'),\n",
        " ('I am very happy', 'میں بہت خوش ہوں'),\n",
        " ('Do you like tea?', 'کیا آپ کو چائے پسند ہے؟'),\n",
        " ('I have finished the book', 'میں نے کتاب مکمل کر لی'),\n",
        " ('This is all very difficult', 'یہ سب کچھ بہت مشکل ہے'),\n",
        " ('I have enough time', 'میرے پاس کافی وقت ہے'),\n",
        " ('How much do you need to travel?', 'تمہیں کتنا سفر کرنا ہے؟'),\n",
        " ('We have started a new project', 'ہم نے ایک نیا منصوبہ شروع کیا ہے'),\n",
        " ('How many people are attending the event?',\n",
        "  'کتنے لوگ اس تقریب میں شامل ہیں؟'),\n",
        " ('Did you complete your work?', 'کیا آپ نے اپنا کام مکمل کیا؟'),\n",
        " ('I bought new clothes', 'میں نے نئے کپڑے خریدے ہیں'),\n",
        " ('He is very hardworking', 'وہ بہت محنتی ہے'),\n",
        " ('My friend helped me', 'میرے دوست نے میری مدد کی'),\n",
        " ('Where do you work?', 'آپ کہاں کام کرتے ہیں؟'),\n",
        " ('My father is an engineer', 'میرے والد ایک انجینئر ہیں'),\n",
        " ('She took care of me', 'اس نے میرا خیال رکھا'),\n",
        " ('Do you have time?', 'کیا آپ کے پاس وقت ہے؟'),\n",
        " ('We are all very happy', 'ہم سب بہت خوش ہیں'),\n",
        " ('I will come in a little while', 'میں ابھی تھوڑی دیر میں آ رہا ہوں'),\n",
        " ('This game is very interesting to me', 'یہ کھیل میرے لیے بہت دلچسپ ہے'),\n",
        " ('How was your day?', 'آپ کا دن کیسا رہا؟'),\n",
        " ('I made a beautiful picture', 'میں نے ایک خوبصورت تصویر بنائی'),\n",
        " ('This place is very beautiful', 'یہ جگہ بہت خوبصورت ہے'),\n",
        " ('What do you need?', 'تمہیں کیا چاہیے؟'),\n",
        " ('We have started a new plan', 'ہم نے ایک نیا منصوبہ شروع کیا ہے'),\n",
        " ('How long have you been here?', 'تم کتنی دیر سے یہاں ہو؟'),\n",
        " ('I want to talk to you', 'میں تم سے بات کرنا چاہتا ہوں'),\n",
        " ('Come with me', 'میرے ساتھ چلیں'),\n",
        " ('This work is very important', 'یہ کام بہت ضروری ہے'),\n",
        " ('He studies very well', 'وہ بہت اچھا پڑھتا ہے'),\n",
        " ('I have bought a new phone', 'میں نے نیا موبائل خریدا ہے'),\n",
        " ('Do you read books?', 'کیا آپ کتابیں پڑھتے ہیں؟'),\n",
        " ('We will go to the park tomorrow', 'ہم کل پارک جائیں گے'),\n",
        " ('I need your help', 'مجھے تمہاری مدد کی ضرورت ہے'),\n",
        " ('Where are you going?', 'آپ کہاں جا رہے ہیں؟'),\n",
        " ('He always helps me', 'وہ ہمیشہ میری مدد کرتا ہے'),\n",
        " ('My parents are very loving', 'میرے والدین بہت محبت کرنے والے ہیں'),\n",
        " ('We have moved to a new city', 'ہم ایک نئے شہر میں منتقل ہو گئے ہیں'),\n",
        " ('We should always tell the truth', 'ہمیں ہمیشہ سچ بولنا چاہیے'),\n",
        " ('What do you have?', 'تمہارے پاس کیا ہے؟'),\n",
        " ('I did a lot of work today', 'میں نے آج بہت کام کیا'),\n",
        " ('I have a new computer', 'میرے پاس ایک نئے کمپیوٹر ہے'),\n",
        " ('How long has it been?', 'تمہیں کتنی دیر ہو گئی؟'),\n",
        " ('He is very cheerful', 'وہ بہت خوش مزاج ہے'),\n",
        " ('Our team worked very hard', 'ہماری ٹیم نے بہت محنت کی'),\n",
        " ('He keeps smiling always', 'وہ ہمیشہ مسکراتا رہتا ہے'),\n",
        " ('We had a great time', 'ہم نے بہت اچھا وقت گزارا'),\n",
        " ('My friends are very helpful', 'میرے دوست بہت مددگار ہیں'),\n",
        " ('My parents are very kind', 'میرے والدین بہت مہربان ہیں'),\n",
        " ('We made a beautiful garden', 'ہم نے ایک خوبصورت باغ بنایا'),\n",
        " ('It is important to solve this issue', 'یہ مسئلہ حل کرنا ضروری ہے'),\n",
        " ('Where do you need to go?', 'تم نے کہاں جانا ہے؟'),\n",
        " ('I am happy to meet you', 'میں تم سے مل کر خوش ہوں'),\n",
        " ('This question is very interesting', 'یہ سوال بہت دلچسپ ہے'),\n",
        " ('I am coming right now', 'میں ابھی آ رہا ہوں'),\n",
        " ('I told you', 'میں نے تمہیں کہا تھا'),\n",
        " ('How are you?', 'تم کیسے ہو؟'),\n",
        " ('Did you buy a new phone?', 'کیا تم نے نیا موبائل خریدا؟'),\n",
        " ('This is a very difficult question', 'یہ بہت مشکل سوال ہے'),\n",
        " ('My friends are very intelligent', 'میرے دوست بہت ذہین ہیں'),\n",
        " ('What do you think about all this?', 'تمہارے خیال میں یہ سب کچھ کیسا ہے؟'),\n",
        " ('We help each other', 'ہم سب ایک دوسرے کی مدد کرتے ہیں'),\n",
        " ('I love you very much', 'میں تم سے بہت محبت کرتا ہوں'),\n",
        " ('Have a pleasant day', 'آپ کا دن خوش گوار گزرے'),\n",
        " ('You are very hardworking', 'تم بہت محنتی ہو'),\n",
        " ('This is very important for us', 'یہ ہمارے لئے بہت اہم ہے'),\n",
        " ('I am coming right now', 'میں ابھی آ رہا ہوں')\n",
        " ]\n",
        "\n",
        " # English-to-Urdu is just reversed\n",
        "ur_en_pairs = [(ur, en) for en, ur in en_ur_pairs]"
      ],
      "metadata": {
        "id": "inRa840EgokX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy dataset: Urdu-English pairs\n",
        "en_ur_pairs = [\n",
        "    (\"hello\", \"ہیلو\"),\n",
        "    (\"how are you\", \"آپ کیسے ہیں\"),\n",
        "    (\"thank you\", \"شکریہ\"),\n",
        "    (\"good morning\", \"صبح بخیر\"),\n",
        "]\n",
        "# English-to-Urdu is just reversed\n",
        "ur_en_pairs = [(ur, en) for en, ur in en_ur_pairs]\n"
      ],
      "metadata": {
        "id": "d9VBjwZeAZCU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer Seq2Seq Code (Single Notebook Script)"
      ],
      "metadata": {
        "id": "UCZLIEP4nuje"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Data Preparation:\n",
        "- We define a small Urdu-English dataset.\n",
        "- Each sentence pair is a translation of the other.\n",
        "2. Vocabulary Building:\n",
        "- We manually assign indices to words.\n",
        "- <pad>: padding, <sos>: start of sentence, <eos>: end of sentence, <unk>: unknown token."
      ],
      "metadata": {
        "id": "ER0lmi962BI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vocab(sentences):\n",
        "    tokens = set()\n",
        "    for sent in sentences:\n",
        "        tokens.update(sent.split())\n",
        "    vocab = {tok: i+4 for i, tok in enumerate(sorted(tokens))}\n",
        "    vocab['<pad>'], vocab['<bos>'], vocab['<eos>'], vocab['<unk>'] = PAD, BOS, EOS, UNK\n",
        "    return vocab\n",
        "\n",
        "src_vocab_en = build_vocab([e for e, _ in en_ur_pairs])\n",
        "tgt_vocab_ur = build_vocab([u for _, u in en_ur_pairs])\n",
        "src_vocab_ur = build_vocab([u for u, _ in ur_en_pairs])\n",
        "tgt_vocab_en = build_vocab([e for _, e in ur_en_pairs])\n",
        "\n",
        "inv_tgt_vocab_ur = {i: tok for tok, i in tgt_vocab_ur.items()}\n",
        "inv_tgt_vocab_en = {i: tok for tok, i in tgt_vocab_en.items()}\n"
      ],
      "metadata": {
        "id": "MASSW7jnnt-j"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Encoding Sentences to Tensors\n",
        "- This function converts a sentence into a fixed-length vector of word indices.\n",
        "- All sentences are padded to max_len.\n",
        "4. Positional Encoding\n",
        "- Since transformers don’t use recurrence (like RNNs), we inject position information using sine/cosine patterns.\n",
        "-Added to word embeddings.\n",
        "5. Transformer Seq2Seq Model\n",
        "- We create a standard encoder-decoder transformer using PyTorch’s built-in module.\n",
        "- src and tgt inputs are passed with positional encodings.\n",
        "- The output is passed through a linear layer to predict the next token.\n",
        "\n",
        "Attention Mechanism\n",
        "The transformer:\n",
        "\n",
        "- Computes self-attention in encoder and decoder to find word relationships in each sentence.\n",
        "- Computes cross-attention in the decoder to relate target words with source words."
      ],
      "metadata": {
        "id": "-e9JGLv42who"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class\n",
        "def tokenize_and_encode(sentence, vocab):\n",
        "    return [vocab.get(tok, UNK) for tok in sentence.split()]\n",
        "\n",
        "class TranslationDataset(Dataset):\n",
        "    def __init__(self, pairs, src_vocab, tgt_vocab, max_len=10):\n",
        "        self.pairs = pairs\n",
        "        self.src_vocab = src_vocab\n",
        "        self.tgt_vocab = tgt_vocab\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self): return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src, tgt = self.pairs[idx]\n",
        "        src_ids = [BOS] + tokenize_and_encode(src, self.src_vocab)[:self.max_len-2] + [EOS]\n",
        "        tgt_ids = [BOS] + tokenize_and_encode(tgt, self.tgt_vocab)[:self.max_len-2] + [EOS]\n",
        "        return torch.tensor(src_ids), torch.tensor(tgt_ids)"
      ],
      "metadata": {
        "id": "0HHRowmj2rt5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collate fn for padding\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = zip(*batch)\n",
        "    src_lens = [len(x) for x in src_batch]\n",
        "    tgt_lens = [len(x) for x in tgt_batch]\n",
        "    max_src, max_tgt = max(src_lens), max(tgt_lens)\n",
        "    src_padded = torch.stack([torch.cat([x, x.new_full((max_src-len(x),), PAD)]) for x in src_batch])\n",
        "    tgt_padded = torch.stack([torch.cat([x, x.new_full((max_tgt-len(x),), PAD)]) for x in tgt_batch])\n",
        "    return src_padded, tgt_padded"
      ],
      "metadata": {
        "id": "Ca48C_gxdNw3"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom Multi-Head Attention with gating\n",
        "class CustomMultiheadAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n",
        "        self.gate = nn.Parameter(torch.rand(1))\n",
        "\n",
        "    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):\n",
        "        attn_output, _ = self.mha(query, key, value,\n",
        "                                  attn_mask=attn_mask,\n",
        "                                  key_padding_mask=key_padding_mask)\n",
        "        # gated residual\n",
        "        return self.gate*attn_output + (1 - self.gate)*query\n",
        "\n",
        "# Encoder and Decoder layers using custom attention\n",
        "class CustomEncoderLayer(nn.Module):\n",
        "    def __init__(self, emb_size, nhead, dim_feedforward=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = CustomMultiheadAttention(emb_size, nhead, dropout)\n",
        "        self.linear1 = nn.Linear(emb_size, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, emb_size)\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
        "        # src: seq_len, batch, emb_size\n",
        "        src2 = self.self_attn(src, src, src,\n",
        "                              attn_mask=src_mask,\n",
        "                              key_padding_mask=src_key_padding_mask)\n",
        "        src = src + self.dropout1(src2)\n",
        "        src = self.norm1(src)\n",
        "        src2 = self.linear2(self.dropout(torch.relu(self.linear1(src))))\n",
        "        src = src + self.dropout2(src2)\n",
        "        return self.norm2(src)\n"
      ],
      "metadata": {
        "id": "6-a_VEd6A-Y8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDecoderLayer(nn.Module):\n",
        "    def __init__(self, emb_size, nhead, dim_feedforward=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = CustomMultiheadAttention(emb_size, nhead, dropout)\n",
        "        self.multihead_attn = CustomMultiheadAttention(emb_size, nhead, dropout)\n",
        "        self.linear1 = nn.Linear(emb_size, dim_feedforward)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, emb_size)\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.norm3 = nn.LayerNorm(emb_size)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, tgt, memory, tgt_mask=None, memory_key_padding_mask=None,\n",
        "                tgt_key_padding_mask=None):\n",
        "        tgt2 = self.self_attn(tgt, tgt, tgt,\n",
        "                              attn_mask=tgt_mask,\n",
        "                              key_padding_mask=tgt_key_padding_mask)\n",
        "        tgt = tgt + self.dropout1(tgt2)\n",
        "        tgt = self.norm1(tgt)\n",
        "        tgt2 = self.multihead_attn(tgt, memory, memory,\n",
        "                                   key_padding_mask=memory_key_padding_mask)\n",
        "        tgt = tgt + self.dropout2(tgt2)\n",
        "        tgt = self.norm2(tgt)\n",
        "        tgt2 = self.linear2(self.dropout(torch.relu(self.linear1(tgt))))\n",
        "        tgt = tgt + self.dropout3(tgt2)\n",
        "        return self.norm3(tgt)\n"
      ],
      "metadata": {
        "id": "I1CnKinudsD-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Training Loop\n",
        "- For each pair:\n",
        "-- Input: Urdu sentence\n",
        "-- Output: English sentence (shifted for decoder input vs target)\n",
        "\n",
        "- We use CrossEntropyLoss (ignoring <pad>) to train the network to predict the next token.\n",
        "- Optimizer: Adam\n",
        "7. Translation Function\n",
        "-Start with <sos>, and use the model to generate one token at a time.\n",
        "\n",
        "-Each output is fed back into the model (greedy decoding)."
      ],
      "metadata": {
        "id": "09hvU93z4ciP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Seq2Seq with custom layers\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self, num_encoder_layers, num_decoder_layers,\n",
        "                 emb_size, nhead, src_vocab_size, tgt_vocab_size,\n",
        "                 dim_feedforward=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_size)\n",
        "        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, emb_size)\n",
        "        self.positional_encoding = PositionalEncoding(emb_size)\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            CustomEncoderLayer(emb_size, nhead, dim_feedforward, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "        self.decoder_layers = nn.ModuleList([\n",
        "            CustomDecoderLayer(emb_size, nhead, dim_feedforward, dropout)\n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "\n",
        "    def encode(self, src, src_mask, src_key_padding_mask):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src) * math.sqrt(self.emb_size))\n",
        "        memory = src_emb.transpose(0, 1)\n",
        "        for layer in self.encoder_layers:\n",
        "            memory = layer(memory, src_mask, src_key_padding_mask)\n",
        "        return memory\n",
        "\n",
        "    def decode(self, tgt, memory, tgt_mask,\n",
        "               tgt_key_padding_mask, memory_key_padding_mask):\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(tgt) * math.sqrt(self.emb_size))\n",
        "        output = tgt_emb.transpose(0, 1)\n",
        "        for layer in self.decoder_layers:\n",
        "            output = layer(output, memory, tgt_mask,\n",
        "                           memory_key_padding_mask,\n",
        "                           tgt_key_padding_mask)\n",
        "        return output\n",
        "\n",
        "    def forward(self, src, tgt, src_mask, tgt_mask,\n",
        "                src_key_padding_mask, tgt_key_padding_mask,\n",
        "                memory_key_padding_mask):\n",
        "        memory = self.encode(src, src_mask, src_key_padding_mask)\n",
        "        decoder_output = self.decode(tgt, memory,\n",
        "                                     tgt_mask,\n",
        "                                     tgt_key_padding_mask,\n",
        "                                     memory_key_padding_mask)\n",
        "        out = self.generator(decoder_output.transpose(0, 1))\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "txXdzD0K4bGg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Evaluation Metrics\n",
        "- BLEU Score: Measures n-gram precision against reference.\n",
        "- METEOR Score: Also considers synonyms and stem matches (more suitable for low-resource languages).\n",
        "- Both are calculated for generated vs reference sentences.\n",
        "\n"
      ],
      "metadata": {
        "id": "MeHVIO5C576t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, emb_size, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, emb_size)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, emb_size, 2).float() *\n",
        "                             (-math.log(10000.0) / emb_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1), :]\n",
        "        return x"
      ],
      "metadata": {
        "id": "tX7bd19J55_u"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask generation\n",
        "\n",
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "# Greedy decode for inference\n",
        "def greedy_decode(model, src_sentence, src_vocab, tgt_vocab,\n",
        "                  inv_tgt_vocab, max_len=10):\n",
        "    model.eval()\n",
        "    tokens = [BOS] + [src_vocab.get(tok, UNK) for tok in src_sentence.split()] + [EOS]\n",
        "    src = torch.tensor(tokens, dtype=torch.long).unsqueeze(0).to(device)\n",
        "    src_mask = torch.zeros(len(tokens), len(tokens)).type(torch.bool).to(device)\n",
        "    src_key_padding_mask = (src == PAD)\n",
        "    memory = model.encode(src, src_mask, src_key_padding_mask)\n",
        "    ys = torch.tensor([[BOS]], dtype=torch.long).to(device)\n",
        "    for i in range(max_len):\n",
        "        tgt_mask = generate_square_subsequent_mask(ys.size(1)).to(device)\n",
        "        out = model.decode(ys, memory, tgt_mask,\n",
        "                           tgt_key_padding_mask=(ys==PAD),\n",
        "                           memory_key_padding_mask=src_key_padding_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        next_word = prob.argmax(1).item()\n",
        "        ys = torch.cat([ys, torch.tensor([[next_word]]).to(device)], dim=1)\n",
        "        if next_word == EOS:\n",
        "            break\n",
        "    translated = [inv_tgt_vocab.get(idx, '<unk>') for idx in ys.squeeze().tolist()]\n",
        "    return ' '.join(translated[1:-1])"
      ],
      "metadata": {
        "id": "yRqlHJ6Xpc_L"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Custom Input Testing\n",
        "- Lets you input your own Urdu or English sentence, get the model's prediction, and evaluate it with BLEU and METEOR.\n",
        "- Results are stored in a CSV file."
      ],
      "metadata": {
        "id": "IXcJqIOr7Azi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and evaluation\n",
        "\n",
        "def train_model(model, dataloader, optimizer, loss_fn, num_epochs=20):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        for src, tgt in dataloader:\n",
        "            src, tgt = src.to(device), tgt.to(device)\n",
        "            tgt_input, tgt_out = tgt[:, :-1], tgt[:, 1:]\n",
        "            src_mask = torch.zeros((src.size(1), src.size(1))).type(torch.bool).to(device)\n",
        "            tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
        "            src_pad_mask = (src == PAD)\n",
        "            tgt_pad_mask = (tgt_input == PAD)\n",
        "            logits = model(src, tgt_input, src_mask, tgt_mask,\n",
        "                           src_pad_mask, tgt_pad_mask, src_pad_mask)\n",
        "            optimizer.zero_grad()\n",
        "            loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
        "\n",
        "\n",
        "def evaluate(model, pairs, src_vocab, tgt_vocab, inv_tgt_vocab):\n",
        "    bleu_scores, meteor_scores = [], []\n",
        "    for src_sent, tgt_sent in pairs:\n",
        "        pred = greedy_decode(model, src_sent, src_vocab, tgt_vocab, inv_tgt_vocab)\n",
        "        ref = [tgt_sent.split()]\n",
        "        bleu_scores.append(sentence_bleu(ref, pred.split()))\n",
        "        meteor_scores.append(meteor_score(ref, pred.split()))\n",
        "    return sum(bleu_scores)/len(bleu_scores), sum(meteor_scores)/len(meteor_scores)\n"
      ],
      "metadata": {
        "id": "GDAxn21ypr7Q"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Urdu->English\n",
        "    dataset_ur_en = TranslationDataset(ur_en_pairs, src_vocab_ur, tgt_vocab_en)\n",
        "    loader_ur_en = DataLoader(dataset_ur_en, batch_size=2, collate_fn=collate_fn)\n",
        "    model_ur_en = Seq2SeqTransformer(2, 2, emb_size=64, nhead=4,\n",
        "                                     src_vocab_size=len(src_vocab_ur),\n",
        "                                     tgt_vocab_size=len(tgt_vocab_en)).to(device)\n",
        "    optimizer_ur_en = optim.Adam(model_ur_en.parameters(), lr=0.001)\n",
        "    loss_fn = nn.CrossEntropyLoss(ignore_index=PAD)\n",
        "    print(\"Training Urdu->English...\")\n",
        "    train_model(model_ur_en, loader_ur_en, optimizer_ur_en, loss_fn, num_epochs=50)\n",
        "    bleu_ur_en, meteor_ur_en = evaluate(model_ur_en, ur_en_pairs,\n",
        "                                        src_vocab_ur, tgt_vocab_en, inv_tgt_vocab_en)\n",
        "    print(f\"Urdu->English BLEU: {bleu_ur_en:.4f}, METEOR: {meteor_ur_en:.4f}\")\n",
        "\n",
        "    # English->Urdu\n",
        "    dataset_en_ur = TranslationDataset(en_ur_pairs, src_vocab_en, tgt_vocab_ur)\n",
        "    loader_en_ur = DataLoader(dataset_en_ur, batch_size=2, collate_fn=collate_fn)\n",
        "    model_en_ur = Seq2SeqTransformer(2, 2, emb_size=64, nhead=4,\n",
        "                                     src_vocab_size=len(src_vocab_en),\n",
        "                                     tgt_vocab_size=len(tgt_vocab_ur)).to(device)\n",
        "    optimizer_en_ur = optim.Adam(model_en_ur.parameters(), lr=0.001)\n",
        "    print(\"Training English->Urdu...\")\n",
        "    train_model(model_en_ur, loader_en_ur, optimizer_en_ur, loss_fn, num_epochs=50)\n",
        "    bleu_en_ur, meteor_en_ur = evaluate(model_en_ur, en_ur_pairs,\n",
        "                                        src_vocab_en, tgt_vocab_ur, inv_tgt_vocab_ur)\n",
        "    print(f\"English->Urdu BLEU: {bleu_en_ur:.4f}, METEOR: {meteor_en_ur:.4f}\")\n",
        "\n",
        "    # Test demo sentences\n",
        "    test_sentences_ur = [\"آپ کیسے ہیں\", \"صبح بخیر\"]\n",
        "    for sent in test_sentences_ur:\n",
        "        print(f\"Urdu->English '{sent}' -> {greedy_decode(model_ur_en, sent, src_vocab_ur, tgt_vocab_en, inv_tgt_vocab_en)}\")\n",
        "    test_sentences_en = [\"good morning\", \"thank you\"]\n",
        "    for sent in test_sentences_en:\n",
        "        print(f\"English->Urdu '{sent}' -> {greedy_decode(model_en_ur, sent, src_vocab_en, tgt_vocab_ur, inv_tgt_vocab_ur)}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KH129s_p__y",
        "outputId": "fd68da6c-fd52-4f9f-97d6-4631f9458b9a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Urdu->English...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5962: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 4.5494\n",
            "Epoch 2, Loss: 3.6744\n",
            "Epoch 3, Loss: 3.1150\n",
            "Epoch 4, Loss: 2.6542\n",
            "Epoch 5, Loss: 2.2812\n",
            "Epoch 6, Loss: 1.9221\n",
            "Epoch 7, Loss: 1.6436\n",
            "Epoch 8, Loss: 1.3533\n",
            "Epoch 9, Loss: 1.1666\n",
            "Epoch 10, Loss: 0.9620\n",
            "Epoch 11, Loss: 0.7592\n",
            "Epoch 12, Loss: 0.6458\n",
            "Epoch 13, Loss: 0.5509\n",
            "Epoch 14, Loss: 0.4834\n",
            "Epoch 15, Loss: 0.3901\n",
            "Epoch 16, Loss: 0.3227\n",
            "Epoch 17, Loss: 0.2678\n",
            "Epoch 18, Loss: 0.2153\n",
            "Epoch 19, Loss: 0.1928\n",
            "Epoch 20, Loss: 0.1476\n",
            "Epoch 21, Loss: 0.1166\n",
            "Epoch 22, Loss: 0.1047\n",
            "Epoch 23, Loss: 0.1205\n",
            "Epoch 24, Loss: 0.0896\n",
            "Epoch 25, Loss: 0.0685\n",
            "Epoch 26, Loss: 0.0620\n",
            "Epoch 27, Loss: 0.0532\n",
            "Epoch 28, Loss: 0.0549\n",
            "Epoch 29, Loss: 0.0370\n",
            "Epoch 30, Loss: 0.0334\n",
            "Epoch 31, Loss: 0.0402\n",
            "Epoch 32, Loss: 0.0329\n",
            "Epoch 33, Loss: 0.0260\n",
            "Epoch 34, Loss: 0.0302\n",
            "Epoch 35, Loss: 0.0246\n",
            "Epoch 36, Loss: 0.0289\n",
            "Epoch 37, Loss: 0.0614\n",
            "Epoch 38, Loss: 0.0604\n",
            "Epoch 39, Loss: 0.0448\n",
            "Epoch 40, Loss: 0.0819\n",
            "Epoch 41, Loss: 0.1503\n",
            "Epoch 42, Loss: 0.2260\n",
            "Epoch 43, Loss: 0.2546\n",
            "Epoch 44, Loss: 0.1757\n",
            "Epoch 45, Loss: 0.1321\n",
            "Epoch 46, Loss: 0.0758\n",
            "Epoch 47, Loss: 0.1385\n",
            "Epoch 48, Loss: 0.1025\n",
            "Epoch 49, Loss: 0.1071\n",
            "Epoch 50, Loss: 0.0996\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.11/dist-packages/nltk/translate/bleu_score.py:577: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Urdu->English BLEU: 0.8898, METEOR: 0.9897\n",
            "Training English->Urdu...\n",
            "Epoch 1, Loss: 4.6388\n",
            "Epoch 2, Loss: 3.8041\n",
            "Epoch 3, Loss: 3.0956\n",
            "Epoch 4, Loss: 2.5871\n",
            "Epoch 5, Loss: 2.1756\n",
            "Epoch 6, Loss: 1.7753\n",
            "Epoch 7, Loss: 1.4849\n",
            "Epoch 8, Loss: 1.2640\n",
            "Epoch 9, Loss: 1.0521\n",
            "Epoch 10, Loss: 0.8263\n",
            "Epoch 11, Loss: 0.6659\n",
            "Epoch 12, Loss: 0.5491\n",
            "Epoch 13, Loss: 0.4457\n",
            "Epoch 14, Loss: 0.3620\n",
            "Epoch 15, Loss: 0.2994\n",
            "Epoch 16, Loss: 0.2314\n",
            "Epoch 17, Loss: 0.2027\n",
            "Epoch 18, Loss: 0.1479\n",
            "Epoch 19, Loss: 0.1443\n",
            "Epoch 20, Loss: 0.1364\n",
            "Epoch 21, Loss: 0.1221\n",
            "Epoch 22, Loss: 0.1641\n",
            "Epoch 23, Loss: 0.1044\n",
            "Epoch 24, Loss: 0.1035\n",
            "Epoch 25, Loss: 0.1094\n",
            "Epoch 26, Loss: 0.0898\n",
            "Epoch 27, Loss: 0.0897\n",
            "Epoch 28, Loss: 0.0610\n",
            "Epoch 29, Loss: 0.0781\n",
            "Epoch 30, Loss: 0.0669\n",
            "Epoch 31, Loss: 0.1108\n",
            "Epoch 32, Loss: 0.0847\n",
            "Epoch 33, Loss: 0.0524\n",
            "Epoch 34, Loss: 0.0621\n",
            "Epoch 35, Loss: 0.0394\n",
            "Epoch 36, Loss: 0.0343\n",
            "Epoch 37, Loss: 0.0554\n",
            "Epoch 38, Loss: 0.0536\n",
            "Epoch 39, Loss: 0.0444\n",
            "Epoch 40, Loss: 0.0281\n",
            "Epoch 41, Loss: 0.0295\n",
            "Epoch 42, Loss: 0.0402\n",
            "Epoch 43, Loss: 0.0658\n",
            "Epoch 44, Loss: 0.0984\n",
            "Epoch 45, Loss: 0.1855\n",
            "Epoch 46, Loss: 0.2776\n",
            "Epoch 47, Loss: 0.2183\n",
            "Epoch 48, Loss: 0.1341\n",
            "Epoch 49, Loss: 0.1469\n",
            "Epoch 50, Loss: 0.0750\n",
            "English->Urdu BLEU: 0.9368, METEOR: 0.9849\n",
            "Urdu->English 'آپ کیسے ہیں' -> My parents are attending\n",
            "Urdu->English 'صبح بخیر' -> How was your me\n",
            "English->Urdu 'good morning' -> آپ کا دن کیسا رہا؟\n",
            "English->Urdu 'thank you' -> آپ ہیں؟\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "URZCZQIHnYPm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}